{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "384855f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "from pyro.contrib.examples.util import MNIST\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pyro.distributions as dist\n",
    "import pyro.contrib.examples.util # patches torchvision\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "# Set the random seed for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bf27cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_loaders(batch_size=128):\n",
    "    root = '.\\data'\n",
    "    trans = transforms.ToTensor()\n",
    "    train_set =MNIST(root = root, train = True, transform = trans, download = True)\n",
    "    test_set = MNIST(root = root, train = False, transform = trans)\n",
    "\n",
    "    kwargs = {'num_workers': 1 }\n",
    "    train_loader = torch.utils.data.DataLoader(dataset = train_set, batch_size= batch_size,\n",
    "                                               shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset = test_set, batch_size=batch_size,\n",
    "                                               shuffle=False, **kwargs)\n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e1c3905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DECODER , 먼저 인코더와 디코더를 어떻게 제공할건지 생각 필요\n",
    "#결국 variational autoencoder는 인코더와 디코더로 구성되어 있고, latent variable에 변분방법이 사용될 뿐\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        #setup the two linear transformations\n",
    "        self.fc1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 784)\n",
    "\n",
    "        #setup the non-linearity\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z):\n",
    "        #define the forward computation on the latent z\n",
    "        #first compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(z))\n",
    "        #return the parameter for the output Bernoulli\n",
    "        #each is of size batch_size *784\n",
    "        loc_img = self.sigmoid(self.fc2(hidden))\n",
    "        #return the output\n",
    "        return loc_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd8d11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder의 정의\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        #setup the two linear transformations\n",
    "        self.fc1 = nn.Linear(784, hidden_dim)\n",
    "        self.fc21 = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc22 = nn.Linear(hidden_dim, z_dim)\n",
    "        #setup the non-linearity\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #define the forward computation on the input x\n",
    "        #first shape the minibatch to have pixels in the rightmost dim\n",
    "        x = x.view(-1, 784)\n",
    "        #then compute the hidden units\n",
    "        hidden = self.softplus(self.fc1(x))\n",
    "        #then return a mean vector and a square root covariance matrix\n",
    "        #each of size batch_size * z_dim\n",
    "        z_loc = self.fc21(hidden)\n",
    "        z_scale = torch.exp(self.fc22(hidden))\n",
    "        return z_loc, z_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d37ba7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model class\n",
    "def model(self,x):\n",
    "    #register PyTorch module 'decoder' with Pyro (prior 정의)\n",
    "    pyro.module(\"decoder\", self.decoder)\n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        #setup hyperparameters ofr prior p(z) , 정규분포포\n",
    "        z_loc = x.new_zeros(torch.Size((x.shape[0], self.z_dim)))\n",
    "        z_scale = x.new_ones(torch.Size((x.shape[0], self.z_dim)))\n",
    "\n",
    "        #sample from prior p(z)\n",
    "        #value will be sampled by guide when computing the ELBO\n",
    "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "        #sample from decoder p(x|z)\n",
    "        loc_img = self.decoder(z)\n",
    "        #sample from the output Bernoulli, 베르누이 분포로 분류 (liklihoood)\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(1), obs=x.view(-1, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "248ff0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the guide ( i.2. variational distribution q(z|x) )\n",
    "def guide(self, x):\n",
    "    #register PyTorch module 'encoder' with Pyro\n",
    "    pyro.module(\"encoder\", self.encoder)\n",
    "    with pyro.plate(\"data\", x.shape[0]):\n",
    "        #compute the parameters of the variational distribution q(z|x)\n",
    "        z_loc, z_scale = self.encoder(x)\n",
    "        #sample from the variational distribution q(z|x)\n",
    "        #value will be used by model when computing the ELBO\n",
    "        z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba8518ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(svi, train_loader):\n",
    "    #initialize loss accumulator\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    #do a training epoch over each mini-batch x returned\n",
    "    #by the data loader\n",
    "    for x, _ in train_loader:\n",
    "        #do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.step(x)\n",
    "\n",
    "    #return epoch loss\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6369542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(svi, test_loader):\n",
    "    #initalize loss accumulato\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    #do a training epoch over each mini-batch x returned\n",
    "    #by the data loader\n",
    "    for x, _ in test_loader:\n",
    "        #do ELBO gradient and accumulate loss\n",
    "        epoch_loss += svi.evaluate_loss(x)\n",
    "\n",
    "    #return epoch loss\n",
    "    normalizer_train = len(test_loader.dataset)\n",
    "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
    "    return total_epoch_loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d55ca7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m test_elbo \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#training loop\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mNUM_EPOCHS\u001b[49m):\n\u001b[0;32m      5\u001b[0m     total_epoch_loss_train \u001b[38;5;241m=\u001b[39m train(svi, train_loader)\n\u001b[0;32m      6\u001b[0m     train_elbo\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mtotal_epoch_loss_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NUM_EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "train_elbo = []\n",
    "test_elbo = []\n",
    "#training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_epoch_loss_train = train(svi, train_loader)\n",
    "    train_elbo.append(-total_epoch_loss_train)\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0 :\n",
    "        #report test diagnostics\n",
    "        total_epoch_loss_test = evaluate(svi, test_loader)\n",
    "        test_elbo.append(-total_epoch_loss_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrostudy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
